{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "03f04d94",
      "metadata": {},
      "source": [
        "# RAID AI Detector (Colab)\n",
        "\n",
        "This notebook sets up the repo, installs dependencies, and runs training or tuning in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "485e7a3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repo (skip if already in your Drive)\n",
        "REPO_URL = \"https://github.com/epicliem/aidetector.git\"\n",
        "REPO_DIR = \"aidetector\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "%cd {REPO_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bc6b04",
      "metadata": {},
      "source": [
        "Replace `REPO_URL` above with your GitHub repo URL. If you're using Google Drive instead, mount Drive and `cd` to the repo folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07955c0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip -q install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4319c09",
      "metadata": {},
      "source": [
        "## TPU setup (Colab)\n",
        "\n",
        "Use a TPU runtime (`Runtime → Change runtime type → TPU`). Install normal deps first, then run this cell to install `torch_xla` and enable XLA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd39e18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you're on TPU, install torch_xla (this will override the torch version)\n",
        "# You may need to restart the runtime after this install.\n",
        "!pip -q install torch==2.1.0 torch_xla==2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "\n",
        "import os\n",
        "os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
        "# os.environ[\"XLA_USE_BF16\"] = \"1\"  # optional: lower memory, faster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38935dd6",
      "metadata": {},
      "source": [
        "Optional: set `HF_TOKEN` for faster downloads (Hugging Face rate limits)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63cd4b7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ[\"HF_TOKEN\"] = \"YOUR_TOKEN_HERE\"\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02cf480d",
      "metadata": {},
      "source": [
        "Optional: create run configs.\n",
        "\n",
        "- `colab.yaml`: smaller run to sanity check.\n",
        "- `tpu.yaml`: full run on TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d0e596",
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "colab_cfg = {\n",
        "    \"dataset\": {\"max_rows\": 20000},\n",
        "    \"training\": {\"batch_size\": 4, \"num_epochs\": 1},\n",
        "    \"mining\": {\"enabled\": False},\n",
        "}\n",
        "\n",
        "with open(\"config/colab.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.safe_dump(colab_cfg, f, sort_keys=False)\n",
        "\n",
        "print(\"Wrote config/colab.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973612ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# TPU config for a bigger run\n",
        "# - device: xla\n",
        "# - no max_rows cap\n",
        "# - keep mining enabled\n",
        "\n",
        "big_tpu_cfg = {\n",
        "    \"dataset\": {\"max_rows\": None},\n",
        "    \"training\": {\n",
        "        \"device\": \"xla\",\n",
        "        \"xla_distributed\": True,\n",
        "        \"xla_cores\": 8,\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 2,\n",
        "    },\n",
        "    \"mining\": {\"enabled\": True, \"start_epoch\": 2},\n",
        "}\n",
        "\n",
        "with open(\"config/tpu.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.safe_dump(big_tpu_cfg, f, sort_keys=False)\n",
        "\n",
        "print(\"Wrote config/tpu.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7edf2c",
      "metadata": {},
      "source": [
        "## H100 Configs\n",
        "\n",
        "If you're using an external H100 instance, use `config/h100.yaml` and `config/h100_tuning.yaml`. In Colab, H100 isn't typically available, so these are just listed here for convenience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea89707a",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478a6f7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# H100 configs (for external GPU instances)\n",
        "# These are generated locally; copy the files to your instance if needed.\n",
        "!ls -lah config/h100.yaml config/h100_tuning.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ccb928",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full config (GPU/CPU)\n",
        "!python scripts/train.py --config config/config.yaml\n",
        "\n",
        "# Smaller Colab config:\n",
        "# !python scripts/train.py --config config/colab.yaml\n",
        "\n",
        "# TPU big run:\n",
        "# !python scripts/train.py --config config/tpu.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3107055",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "Note: keep `training.xla_distributed: false` for tuning so metrics can be returned per trial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43d13f59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full config (GPU/CPU)\n",
        "!python scripts/tune.py --config config/config.yaml\n",
        "\n",
        "# Smaller Colab config:\n",
        "# !python scripts/tune.py --config config/colab.yaml\n",
        "\n",
        "# TPU tuning: use a config with xla_distributed: false\n",
        "# (do not use config/tpu.yaml as-is for tuning)\n",
        "# !python scripts/tune.py --config config/tpu.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ccce22",
      "metadata": {},
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7931eb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir outputs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
